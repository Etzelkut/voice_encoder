{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wav2vec2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ce0c1280d8ca498eaf2320cc440bfbfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d623e6b883b54f8fa5bae2d7299bc402",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_07a7e641eaf34cf09339bf902124a979",
              "IPY_MODEL_0f1c0d1d01b44ba593ab93eb86fa99e2"
            ]
          }
        },
        "d623e6b883b54f8fa5bae2d7299bc402": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "07a7e641eaf34cf09339bf902124a979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_462e487bb9d34f98ab8381822c1a7c42",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 843,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 843,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fc7cf81aa81d4e4e9f4d3ed813b453da"
          }
        },
        "0f1c0d1d01b44ba593ab93eb86fa99e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8505776539f14b6195ad3f2857c8c738",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 843/843 [00:00&lt;00:00, 1.16kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7873a801490e436e8051f067a02697ec"
          }
        },
        "462e487bb9d34f98ab8381822c1a7c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fc7cf81aa81d4e4e9f4d3ed813b453da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8505776539f14b6195ad3f2857c8c738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7873a801490e436e8051f067a02697ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "950370f2e9c64f4584818e7e7e6c3502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d99beac57dd64bfc86a9803885497b54",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_401cc1fa6cee4ef5bf9c9fb491763b0a",
              "IPY_MODEL_a369ded148d849eaab9975d7da3cb76d"
            ]
          }
        },
        "d99beac57dd64bfc86a9803885497b54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "401cc1fa6cee4ef5bf9c9fb491763b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ac2b1e9bf907433cb6dcd0f3a95064ff",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 377667514,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 377667514,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b3f20b788a374c24aea270ac6ca1e48d"
          }
        },
        "a369ded148d849eaab9975d7da3cb76d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a9484b0bdcd04e849c2467624becd690",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 378M/378M [00:26&lt;00:00, 14.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_994bc8a0a920400b877a809b2907d42f"
          }
        },
        "ac2b1e9bf907433cb6dcd0f3a95064ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b3f20b788a374c24aea270ac6ca1e48d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a9484b0bdcd04e849c2467624becd690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "994bc8a0a920400b877a809b2907d42f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "USYanXCVmNle"
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItJB-TAW1-vk"
      },
      "source": [
        "%%capture\n",
        " #!cat /content/drive/MyDrive/senior_sound/celeb1/vox1_dev* > /content/vox1_dev_wav.zip\n",
        "!unzip /content/drive/MyDrive/senior_sound/celeb1/vox1_dev_wav.zip -d /content/data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDBQmCvrFOuR"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small_960h.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I9VHkJe4UUS"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install adabelief-pytorch==0.2.0\n",
        "!pip install ranger-adabelief==0.1.0\n",
        "\n",
        "#!pip install torchtext#==0.8.1\n",
        "#==0.7.0\n",
        "!pip install torchaudio\n",
        "#==1.2.2\n",
        "!pip install pytorch-lightning\n",
        "#==3.4.0\n",
        "!pip install comet-ml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPs_6r0SD4Ik"
      },
      "source": [
        "\"\"\"\n",
        "import os\n",
        "path = \"/content/data/wav\"\n",
        "\n",
        "speakers = os.listdir(path)\n",
        "speakers_file_list = {}\n",
        "utterences = 0\n",
        "speakers_number = len(speakers)\n",
        "for i in range(len(speakers)):\n",
        "  id_path = os.path.join(path, speakers[i])\n",
        "  speakers_file_list[speakers[i]] = getListOfFiles(id_path)\n",
        "  utterences += len(speakers_file_list[speakers[i]])\n",
        "\n",
        "listsize = []\n",
        "lenutterences = []\n",
        "samplerates = []\n",
        "all_audio = []\n",
        "\n",
        "for key, values in speakers_file_list.items():\n",
        "  listsize.append(len(values))\n",
        "  xrate = 0\n",
        "  secsize = 0\n",
        "  for value in values:\n",
        "    sample_rate, audio = wavfile.read(value)\n",
        "    all_audio.append(audio.shape[0])\n",
        "    secsize += audio.shape[0] / sample_rate\n",
        "    if xrate == 0:\n",
        "      xrate = sample_rate\n",
        "    else:\n",
        "      if sample_rate != xrate:\n",
        "        print(values)\n",
        "        print(value)\n",
        "        break\n",
        "  samplerates.append(sample_rate)\n",
        "  lenutterences.append(secsize)\n",
        "\n",
        "\n",
        "print(min(lenutterences), lenutterences.index(min(lenutterences)), lenutterences[161]) # (267.88281249999994, 726, 361.04281249999997)\n",
        "print(min(listsize), listsize.index(min(listsize)), listsize[726]) # (45, 161, 45)\n",
        "\n",
        "all_audio2 = np.array(all_audio)\n",
        "new_l = np.around(all_audio2/sample_rate, decimals=1)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(new_l, bins=np.arange(new_l.min(), new_l.max()+1))\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUw8skPM2xGz"
      },
      "source": [
        "from comet_ml import Experiment\n",
        "from pytorch_lightning.loggers import CometLogger\n",
        "#from pytorch_model_summary import summary\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import seed_everything\n",
        "\n",
        "from scipy.io import wavfile\n",
        "import os\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import torch\n",
        "from transformers import Wav2Vec2PreTrainedModel, Wav2Vec2Model, Wav2Vec2Processor\n",
        "\n",
        "from ge2e import GE2ELoss\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def seed_e(seed_value):\n",
        "  seed_everything(seed_value)\n",
        "  random.seed(seed_value)\n",
        "  np.random.seed(seed_value) \n",
        "  torch.manual_seed(seed_value)\n",
        "  torch.cuda.manual_seed(seed_value)\n",
        "  torch.cuda.manual_seed_all(seed_value)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfe_e731blhT"
      },
      "source": [
        "def getListOfFiles(dirName):\n",
        "    # create a list of file and sub directories \n",
        "    # names in the given directory \n",
        "    listOfFile = os.listdir(dirName)\n",
        "    allFiles = list()\n",
        "    # Iterate over all the entries\n",
        "    for entry in listOfFile:\n",
        "        # Create full path\n",
        "        fullPath = os.path.join(dirName, entry)\n",
        "        # If entry is a directory then get the list of files in this directory \n",
        "        if os.path.isdir(fullPath):\n",
        "            allFiles = allFiles + getListOfFiles(fullPath)\n",
        "        else:\n",
        "            allFiles.append(fullPath)\n",
        "                \n",
        "    return allFiles        "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KSr8Ozw71xG"
      },
      "source": [
        "class VoxCeleb(Dataset):\n",
        "    \n",
        "    def __init__(self, path, sampling_rate = 16000, max_seconds = 8, utterences_per_speaker = 2, full_data = True, window_size = None, step_size = None, \n",
        "                 shuffle=True): # path = \"/content/data/wav/\"\n",
        "\n",
        "      # 3 - 4.1 seconds max\n",
        "\n",
        "      if full_data:\n",
        "        assert utterences_per_speaker < 45 # do max 40\n",
        "      else:\n",
        "        assert utterences_per_speaker < (267/step_size) - 1 # do max 40 if step size = 2 sec and window = 4 sec\n",
        "      \n",
        "      self.sampling_rate = sampling_rate\n",
        "      self.max_seconds = max_seconds\n",
        "\n",
        "      self.M = utterences_per_speaker\n",
        "      self.full_data = full_data\n",
        "      \n",
        "      self.window_size = window_size\n",
        "      self.step_size = step_size\n",
        "\n",
        "      self.shuffle = shuffle\n",
        "\n",
        "      self.speakers = os.listdir(path)\n",
        "      self.speakers_file_list = {}\n",
        "      self.utterences = 0\n",
        "      self.speakers_number = len(self.speakers)\n",
        "      for i in range(len(self.speakers)):\n",
        "        id_path = os.path.join(path, self.speakers[i])\n",
        "        self.speakers_file_list[self.speakers[i]] = getListOfFiles(id_path)\n",
        "        self.utterences += len(self.speakers_file_list[self.speakers[i]])\n",
        "    \n",
        "    def sample_audio(self, audio):\n",
        "      seconds = audio.shape[0] / self.sampling_rate\n",
        "      max_samples = int(self.sampling_rate * self.max_seconds)\n",
        "      if seconds > self.max_seconds:\n",
        "        start_audio = random.sample(range(0, audio.shape[0] - max_samples), 1)[0]\n",
        "        audio = audio[start_audio:start_audio + max_samples]\n",
        "      return audio\n",
        "\n",
        "    def read_audio(self, path):\n",
        "      audio, sr = sf.read(path)\n",
        "      if sr != self.sampling_rate:\n",
        "        print(path)\n",
        "        raise Exception(\"sampling rate broken\")\n",
        "      audio = self.sample_audio(audio)\n",
        "      return audio\n",
        "\n",
        "    def __len__(self):\n",
        "      return self.speakers_number\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      \n",
        "      if self.shuffle:\n",
        "        selected_speaker = random.sample(self.speakers, 1)[0]  # select random speaker\n",
        "      else:\n",
        "        selected_speaker = self.speakers[idx]   \n",
        "\n",
        "      if self.full_data:\n",
        "        list_of_audio = random.sample(self.speakers_file_list[selected_speaker], self.M)\n",
        "        list_of_audio = list(map(self.read_audio, list_of_audio))\n",
        "      else:\n",
        "        raise Exception(\"Only full data avaulable now\")\n",
        "\n",
        "        # load utterance spectrogram of selected speaker\n",
        "\n",
        "        # select M utterances per speaker\n",
        "\n",
        "        # utterances of a speaker [batch(M), n_mels, frames]\n",
        "\n",
        "        # transpose [batch, frames, n_mels]\n",
        "      return list_of_audio, [selected_speaker]*len(list_of_audio)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI4R9RKbNeeM"
      },
      "source": [
        "def collate_fn_vox(batch, processor, sampling_rate, max_length = None):\n",
        "  \n",
        "  speakers_number = len(batch)\n",
        "\n",
        "  connected_audio_list = []\n",
        "  speakers = []\n",
        "  for list_of_audio, selected_speaker in batch:\n",
        "    connected_audio_list += list_of_audio\n",
        "    speakers+=selected_speaker\n",
        "  \n",
        "  input_values = processor(connected_audio_list, padding = True, max_length = max_length, return_attention_mask = True, sampling_rate = sampling_rate, return_tensors=\"pt\")\n",
        "  speakers.append(speakers_number)\n",
        "\n",
        "  return input_values.input_values, input_values.attention_mask, speakers"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGrGN8Zx6O-m"
      },
      "source": [
        "class Dataset_vox(pl.LightningDataModule):\n",
        "    def __init__(self, conf, *args, **kwargs): #*args, **kwargs hparams, steps_per_epoch\n",
        "      super().__init__()\n",
        "      self.hparams = conf\n",
        "      self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "    def prepare_data(self):\n",
        "      print(\"can add download here\")\n",
        "    \n",
        "    def setup(self):\n",
        "      \n",
        "      dataset = VoxCeleb(self.hparams[\"path\"], sampling_rate=self.hparams[\"sampling_rate\"], max_seconds=self.hparams[\"max_seconds\"], \n",
        "                         utterences_per_speaker=self.hparams[\"utterences_per_speaker\"], full_data=self.hparams[\"full_data\"], \n",
        "                         window_size=self.hparams[\"window_size\"], step_size=self.hparams[\"step_size\"], shuffle=self.hparams[\"shuffle_speakers\"])\n",
        "\n",
        "      size_of_main = len(dataset)\n",
        "      self.dataset_train, self.dataset_val = torch.utils.data.random_split(dataset, \n",
        "                                              [int(size_of_main*0.9), size_of_main - int(size_of_main*0.9)], \n",
        "                                              generator=torch.Generator().manual_seed(42))\n",
        "      self.dataset_test = None\n",
        "    def train_dataloader(self):\n",
        "      data_train = DataLoader(self.dataset_train, batch_size=self.hparams[\"number_of_speakers\"], num_workers=self.hparams[\"num_workers\"], \n",
        "                              shuffle=self.hparams[\"dataloader_shuffle\"], \n",
        "                              collate_fn = lambda x:collate_fn_vox(x, self.processor, self.hparams[\"sampling_rate\"], max_length = self.hparams[\"max_length\"])\n",
        "                              )\n",
        "      return data_train\n",
        "\n",
        "    def val_dataloader(self):\n",
        "      val = DataLoader(self.dataset_val, batch_size=self.hparams[\"number_of_speakers\"], num_workers=self.hparams[\"num_workers\"], \n",
        "                              shuffle=False, \n",
        "                              collate_fn = lambda x:collate_fn_vox(x, self.processor, self.hparams[\"sampling_rate\"], max_length = self.hparams[\"max_length\"])\n",
        "                              )\n",
        "      return val\n",
        "\n",
        "    def test_dataloader(self):\n",
        "      test = self.dataset_test\n",
        "      return test"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "3PAKfVfyDkpe",
        "outputId": "191000e1-b8f9-4d16-a977-281478943522"
      },
      "source": [
        "class Voice_Encoder_pl(pl.LightningModule):\n",
        "    def __init__(self, re_dict, *args, **kwargs): #*args, **kwargs hparams, steps_per_epoch\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(re_dict)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.model_params = self.hparams[\"model_params\"]\n",
        "        self.learning_params = self.hparams[\"training\"]\n",
        "\n",
        "        #self.swa_model = None\n",
        "        #self.swa_mode = False\n",
        "\n",
        "        #print(\"mixup set: \", self.learning_params[\"mixup\"])\n",
        "        #if self.learning_params[\"data_dropout\"]:\n",
        "        #    print(\"data_dropout activated\")\n",
        "        #    self.time_drop = torchaudio.transforms.TimeMasking(time_mask_param=self.learning_params[\"time_l\"])\n",
        "        # self.check_random_mixup = False\n",
        "        self.feature_extractor = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "        self.fc1 = nn.Linear(in_features = 768, out_features = self.model_params[\"fc1_dim\"])\n",
        "        self.avpool = nn.AvgPool1d(5, stride=3)\n",
        "        self.fc2 = nn.Linear(in_features = self.model_params[\"fc1_dim\"], out_features = self.model_params[\"fc2_dim\"])\n",
        "        self.fc3 = nn.Linear(in_features = self.model_params[\"fc2_dim\"], out_features = self.model_params[\"embeding\"])\n",
        "\n",
        "        self.criterion = GE2ELoss(init_w=10.0, init_b=-5.0, loss_method='softmax')\n",
        "\n",
        "    def forward(self, audio, attention_mask):\n",
        "      if self.learning_params[\"block\"] or (self.current_epoch < self.learning_params[\"start_learning_feature_epoch\"]):\n",
        "        self.feature_extractor.eval()\n",
        "        with torch.no_grad():\n",
        "          hidden = self.feature_extractor(audio, attention_mask).last_hidden_state\n",
        "      else:\n",
        "        self.feature_extractor.train()\n",
        "        hidden = self.feature_extractor(audio, attention_mask).last_hidden_state\n",
        "\n",
        "      hidden = self.fc1(hidden)\n",
        "      hidden = hidden.transpose(1,2).contiguous()\n",
        "      hidden = self.avpool(hidden)\n",
        "      hidden = hidden.transpose(1,2).contiguous()\n",
        "      hidden = self.fc2(hidden)\n",
        "      hidden = torch.sum(hidden, dim = 1)\n",
        "      hidden = self.fc3(hidden)\n",
        "\n",
        "      return hidden\n",
        "    \n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        if self.learning_params[\"optimizer\"] == \"belief\":\n",
        "            optimizer =  AdaBelief(self.parameters(), lr = self.learning_params[\"lr\"], eps = self.learning_params[\"eplison_belief\"],\n",
        "                                    weight_decouple = self.learning_params[\"weight_decouple\"], \n",
        "                                    weight_decay = self.learning_params[\"weight_decay\"], rectify = self.learning_params[\"rectify\"])\n",
        "        elif self.learning_params[\"optimizer\"] == \"ranger_belief\":\n",
        "            optimizer = RangerAdaBelief(self.parameters(), lr = self.learning_params[\"lr\"], eps = self.learning_params[\"eplison_belief\"],\n",
        "                                       weight_decouple = self.learning_params[\"weight_decouple\"],  weight_decay = self.learning_params[\"weight_decay\"],)\n",
        "        elif self.learning_params[\"optimizer\"] == \"adam\":\n",
        "            optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_params[\"lr\"])\n",
        "        elif self.learning_params[\"optimizer\"] == \"adamW\":\n",
        "            optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_params[\"lr\"])        \n",
        "\n",
        "        if self.learning_params[\"add_sch\"]:\n",
        "            #CosineScheduler(20, warmup_steps=5, base_lr=0.3, final_lr=0.01)\n",
        "            #MultiStepLR(trainer, milestones=[15, 30], gamma=0.5)\n",
        "            lr_scheduler = {'scheduler': torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
        "\t                                                                        max_lr=self.learning_params[\"lr\"],\n",
        "\t                                                                        steps_per_epoch=self.hparams.steps_per_epoch, #int(len(train_loader))\n",
        "\t                                                                        epochs=self.learning_params[\"epochs\"],\n",
        "\t                                                                        anneal_strategy='linear'),\n",
        "                        'name': 'lr_scheduler_lr',\n",
        "                        'interval': 'step', # or 'epoch'\n",
        "                        'frequency': 1,\n",
        "                        }\n",
        "            print(\"sch added\")\n",
        "            return [optimizer], [lr_scheduler]\n",
        "        return optimizer\n",
        "    \n",
        "    def loss_function(self, x, speakers):\n",
        "      # N, M, D: N - Number of speakers in a batch, M - Number of utterances for each speaker, D - d-vector\n",
        "      b, d = x.shape\n",
        "      speakers_number = speakers[-1] # N - Number of speakers in a batch, \n",
        "\n",
        "      x = x.view(speakers_number, -1, d)\n",
        "      loss = self.criterion(x)\n",
        "\n",
        "      return loss\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        #also Manual optimization exist\n",
        "        x, mask, speakers = batch\n",
        "        output = self(x, mask)\n",
        "        loss = self.loss_function(output, speakers)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True) # prog_bar=True\n",
        "        return loss\n",
        "\n",
        "    #copied\n",
        "    def get_lr_inside(self, optimizer):\n",
        "        for param_group in optimizer.param_groups:\n",
        "            return param_group['lr']\n",
        "\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        self.log('epoch_now', self.current_epoch, on_step=False, on_epoch=True, logger=True)\n",
        "        (oppp) =  self.optimizers(use_pl_optimizer=True)\n",
        "        self.log('lr_now', self.get_lr_inside(oppp), on_step=False, on_epoch=True, logger=True)\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, mask, speakers = batch\n",
        "        output = self(x, mask)\n",
        "        loss = self.loss_function(output, speakers)\n",
        "\n",
        "        self.log('val_loss', loss, on_step=False, on_epoch=True, logger=True) #prog_bar=True,\n",
        "        return {'val_loss': loss}\n",
        "\n",
        "\"\"\"\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \n",
        "        x, mask, speakers = batch\n",
        "        output = self(x, mask)\n",
        "        loss = self.loss_function(output, speakers)\n",
        "\n",
        "        return {'test_loss': loss, #!!!!!!!!!!\n",
        "\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
        "\n",
        "        self.log('test_f1_score_weighted', f1_scored_w, on_step=False, on_epoch=True, logger=True) #prog_bar=True,\n",
        "        self.log('test_f1_score_macro', f1_scored_m, on_step=False, on_epoch=True,  logger=True) #prog_bar=True,\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n    def test_step(self, batch, batch_idx):\\n        \\n        x, mask, speakers = batch\\n        output = self(x, mask)\\n        loss = self.loss_function(output, speakers)\\n\\n        return {'test_loss': loss, #!!!!!!!!!!\\n\\n\\n    def test_epoch_end(self, outputs):\\n        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\\n\\n        self.log('test_f1_score_weighted', f1_scored_w, on_step=False, on_epoch=True, logger=True) #prog_bar=True,\\n        self.log('test_f1_score_macro', f1_scored_m, on_step=False, on_epoch=True,  logger=True) #prog_bar=True,\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lunFQBnkD2-Z"
      },
      "source": [
        "# data params\n",
        "data_params = {\n",
        "    \"path\": \"/content/data/wav/\",\n",
        "    \"sampling_rate\": 16000,\n",
        "    \"max_seconds\": 8, \n",
        "    \"max_length\": None,\n",
        "    \"utterences_per_speaker\": 5, # M\n",
        "    \"full_data\": True,\n",
        "    \"window_size\": None,\n",
        "    \"step_size\": None,\n",
        "    \"shuffle_speakers\": True,\n",
        "    \"number_of_speakers\": 10, # N\n",
        "    \"num_workers\": 2,\n",
        "    \"dataloader_shuffle\": True,\n",
        "}\n",
        "\n",
        "# model params\n",
        "\n",
        "model_params = {\n",
        "    \"fc1_dim\": 512,\n",
        "    \"fc2_dim\": 512,\n",
        "    \"embeding\": 256\n",
        "}\n",
        "\n",
        "\n",
        "# learning params\n",
        "\n",
        "learning_params = {\n",
        "    \"block\": True,\n",
        "    \"start_learning_feature_epoch\": None,\n",
        "    \n",
        "    \"optimizer\": \"belief\", # \"belief\", \"ranger_belief\", \"adam\", adamW\n",
        "    \"lr\": 3e-4, #\n",
        "    \"eplison_belief\": 1e-16,\n",
        "    \"beta\": [0.9, 0.999], # not used\n",
        "    \"weight_decouple\": True, \n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"rectify\": True,\n",
        "    #\n",
        "    \"add_sch\": False,\n",
        "    #\n",
        "    \"epochs\": 10, #\n",
        "}\n",
        "\n",
        "hparams_encoder = {\n",
        "    \"model_params\": model_params,\n",
        "    \"training\": learning_params,\n",
        "    \"data_params\": data_params,\n",
        "}"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZq-4iJ3BTo8",
        "outputId": "8db8d385-2b09-4dc5-a2d8-5b25c005ed9d"
      },
      "source": [
        "dataset_pl = Dataset_vox(hparams_encoder[\"data_params\"])\n",
        "dataset_pl.prepare_data()\n",
        "dataset_pl.setup()\n",
        "\n",
        "for batch in dataset_pl.train_dataloader():\n",
        "  x, mask, speakers = batch\n",
        "  print(x, mask, speakers)\n",
        "  print(x.shape, mask.shape)\n",
        "  break\n",
        "del dataset_pl"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "can add download here\n",
            "tensor([[-0.4486, -0.5570, -0.5912,  ..., -0.0089,  0.2642,  0.3124],\n",
            "        [ 0.6341,  0.6820,  0.5136,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0612,  0.2578,  0.1526,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 3.0502,  2.3039,  1.6878,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 5.0606,  5.7087,  4.4404,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0482,  0.0406,  0.0335,  ..., -0.2669, -0.2357, -0.2463]]) tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]]) ['id10656', 'id10656', 'id10656', 'id10656', 'id10656', 'id10581', 'id10581', 'id10581', 'id10581', 'id10581', 'id10677', 'id10677', 'id10677', 'id10677', 'id10677', 'id10535', 'id10535', 'id10535', 'id10535', 'id10535', 'id11015', 'id11015', 'id11015', 'id11015', 'id11015', 'id10435', 'id10435', 'id10435', 'id10435', 'id10435', 'id10245', 'id10245', 'id10245', 'id10245', 'id10245', 'id10710', 'id10710', 'id10710', 'id10710', 'id10710', 'id11049', 'id11049', 'id11049', 'id11049', 'id11049', 'id11062', 'id11062', 'id11062', 'id11062', 'id11062', 10]\n",
            "torch.Size([50, 128000]) torch.Size([50, 128000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "ce0c1280d8ca498eaf2320cc440bfbfc",
            "d623e6b883b54f8fa5bae2d7299bc402",
            "07a7e641eaf34cf09339bf902124a979",
            "0f1c0d1d01b44ba593ab93eb86fa99e2",
            "462e487bb9d34f98ab8381822c1a7c42",
            "fc7cf81aa81d4e4e9f4d3ed813b453da",
            "8505776539f14b6195ad3f2857c8c738",
            "7873a801490e436e8051f067a02697ec",
            "950370f2e9c64f4584818e7e7e6c3502",
            "d99beac57dd64bfc86a9803885497b54",
            "401cc1fa6cee4ef5bf9c9fb491763b0a",
            "a369ded148d849eaab9975d7da3cb76d",
            "ac2b1e9bf907433cb6dcd0f3a95064ff",
            "b3f20b788a374c24aea270ac6ca1e48d",
            "a9484b0bdcd04e849c2467624becd690",
            "994bc8a0a920400b877a809b2907d42f"
          ]
        },
        "id": "4HSWoDlQDY-c",
        "outputId": "5a996c0c-5f95-4502-adb7-cd05eab3781f"
      },
      "source": [
        "re_dict_check = hparams_encoder.copy()\n",
        "model = Voice_Encoder_pl(re_dict_check) #Updated_Re_model(re_dict_updated) #Updated_old_Re_model(re_dict_check)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce0c1280d8ca498eaf2320cc440bfbfc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=843.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "950370f2e9c64f4584818e7e7e6c3502",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=377667514.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY5UGyzbHfCd",
        "outputId": "6c6e9522-6646-4485-ff43-67d7cdb83ea7"
      },
      "source": [
        "hidden = model.forward(x, mask)\n",
        "print(hidden.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN1dklcGDZBX"
      },
      "source": [
        "seed_v = 42\n",
        "root_dir = \"/content/drive/MyDrive/senior_sound/celeb1/encoder_weights\"\n",
        "naming = \"encoder_try_1\"\n",
        "\n",
        "seed_e(seed_v)\n",
        "\n",
        "comet_logger = CometLogger(\n",
        "  save_dir='/content/log/',\n",
        "  api_key=\"23CU99n7TeyZdPeegNDlQ5aHf\",\n",
        "  project_name=\"encoder-voice\",\n",
        "  workspace=\"etzelkut\",\n",
        "  # rest_api_key=os.environ[\"COMET_REST_KEY\"], # Optional\n",
        "  experiment_name = naming, # Optional\n",
        ")\n",
        "  #\n",
        "\n",
        "dataset_pl = Dataset_vox(hparams_encoder[\"data_params\"])\n",
        "dataset_pl.prepare_data()\n",
        "dataset_pl.setup()\n",
        "steps_per_epoch = int(len(dataset_pl.train_dataloader()))\n",
        "print(steps_per_epoch)\n",
        "proj_a = Voice_Encoder_pl(hparams_encoder, steps_per_epoch = steps_per_epoch)\n",
        "\n",
        "trainer = Trainer(#callbacks=[lr_monitor],\n",
        "                    logger=comet_logger,\n",
        "                    gpus=1,\n",
        "                    profiler=True,\n",
        "                    #auto_lr_find=True, #set hparams\n",
        "                    #gradient_clip_val=0.5,\n",
        "                    check_val_every_n_epoch=1,\n",
        "                    #early_stop_callback=True,\n",
        "                    max_epochs = re_dict[\"training\"][\"epochs\"],\n",
        "                    progress_bar_refresh_rate = 0,\n",
        "                    deterministic=True,)\n",
        "\n",
        "trainer.fit(proj_a, dataset_pl)\n",
        "\n",
        "checkpoint_name = os.path.join(root_dir, naming + '.ckpt')\n",
        "trainer.save_checkpoint(checkpoint_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dUI2FZpDZGd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XXVWCHo9e2P"
      },
      "source": [
        "\"\"\"\n",
        "from IPython.display import Audio\n",
        "display(Audio(signal, rate=dataset.sampling_rate))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}